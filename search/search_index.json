{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>EvoProtGrad is a Python package for sampling mutations near a wild type protein. Directed evolution on a protein sequence with gradient-based discrete Markov chain monte carlo (MCMC) enables users to compose their custom protein models that map sequence to function with various pretrained models, including protein language models (PLMs). The library is designed to natively integrate with \ud83e\udd17 HuggingFace and supports PLMs from the transformers library.</p> <p>The underlying search technique is based on a variant of discrete MCMC that uses gradients of a differentiable compositional target function to rapidly explore a protein's fitness landscape in sequence space.  We allow users to compose their own custom target function for MCMC by leveraging the Product of Experts MCMC paradigm. Each model is an \"expert\" that contributes its own knowledge about the protein's fitness landscape to the overall target function. Our MCMC sampler is designed to be more efficient and effective than brute force and random search while maintaining most of the generality and flexibility.</p> <p>See our publication for more details.</p>"},{"location":"#installation","title":"Installation","text":"<p>EvoProtGrad is available on PyPI and can be installed with pip:</p> <pre><code>pip install evo_prot_grad\n</code></pre> <p>If you wish to run tests or register a new expert model with EvoProtGrad, please clone this repo and install in editable mode as follows:</p> <pre><code>git clone https://github.com/NREL/EvoProtGrad.git\ncd EvoProtGrad\npip install -e .\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Create a <code>ProtBERT</code> expert from a pretrained HuggingFace protein language model (PLM) using <code>evo_prot_grad.get_expert</code>:</p> <pre><code>import evo_prot_grad\nprot_bert_expert = evo_prot_grad.get_expert('bert', temperature = 1.0)\n</code></pre> <p>The default BERT-style PLM in <code>EvoProtGrad</code> is <code>Rostlab/prot_bert</code>. Normally, we would need to also specify the model and tokenizer. When using a default PLM expert, we automatically pull these from the HuggingFace Hub. The temperature parameter rescales the expert scores and can be used to trade off the importance of different experts. For masked language models like <code>prot_bert</code>, we score variant sequences with the sum of amino acid log probabilities by default.</p> <p>Then, create an instance of <code>DirectedEvolution</code> and run the search, returning a list of the best variant per Markov chain (as measured by the <code>prot_bert</code> expert):</p> <pre><code>variants, scores = evo_prot_grad.DirectedEvolution(\nwt_fasta = 'test/gfp.fasta',    # path to wild type fasta file\noutput = 'best',                # return best, last, all variants    \nexperts = [prot_bert_expert],   # list of experts to compose\nparallel_chains = 1,            # number of parallel chains to run\nn_steps = 20,                   # number of MCMC steps per chain\nmax_mutations = 10,             # maximum number of mutations per variant\nverbose = True                  # print debug info to command line\n)()\n</code></pre> <p>We provide a few  experts in <code>evo_prot_grad/experts</code> that you can use out of the box, such as:</p> <p>Protein Language Models (PLMs)</p> <ul> <li><code>bert</code>, BERT-style PLMs, default: <code>Rostlab/prot_bert</code></li> <li><code>causallm</code>, CausalLM-style PLMs, default: <code>lightonai/RITA_s</code></li> <li><code>esm</code>, ESM-style PLMs, default: <code>facebook/esm2_t6_8M_UR50D</code></li> </ul> <p>Potts models</p> <ul> <li><code>evcouplings</code></li> </ul> <p>and an generic expert for supervised downstream regression models</p> <ul> <li><code>onehot_downstream_regression</code></li> </ul> <p>See <code>demo.ipynb</code> to get started right away in a Jupyter notebook.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use EvoProtGrad in your research, please cite the following publication:</p> <pre><code>@article{emami2023plug,\ntitle={Plug \\&amp; play directed evolution of proteins with gradient-based discrete MCMC},\nauthor={Emami, Patrick and Perreault, Aidan and Law, Jeffrey and Biagioni, David and John, Peter St},\njournal={Machine Learning: Science and Technology},\nvolume={4},\nnumber={2},\npages={025014},\nyear={2023},\npublisher={IOP Publishing}\n}\n</code></pre>"},{"location":"api/evo_prot_grad/","title":"evo_prot_grad","text":""},{"location":"api/evo_prot_grad/#get_expert","title":"get_expert","text":""},{"location":"api/evo_prot_grad/#evo_prot_grad.get_expert","title":"<code>evo_prot_grad.get_expert(expert_name: str, temperature: float = 1.0, model: Optional[nn.Module] = None, tokenizer: Optional[Union[ExpertTokenizer, PreTrainedTokenizerBase]] = None, device: str = 'cpu', use_without_wildtype: bool = False) -&gt; Expert</code>","text":"<p>Current supported expert types (to pass to argument <code>expert_name</code>):</p> <pre><code>- `bert`\n- `causallm`\n- `esm`\n- `evcouplings`\n- `onehot_downstream_regression`\n</code></pre> <p>Customize the expert by specifying the model and tokenizer.  For example:</p> <pre><code>from evo_prot_grad.experts import get_expert\nfrom transformers import AutoTokenizer, EsmForMaskedLM\nexpert = get_expert(\nexpert_name = 'esm',\nmodel = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t36_3B_UR50D\"),\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t36_3B_UR50D\"),\ndevice = 'cuda'\n)   \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>expert_name</code> <code>str</code> <p>Name of the expert to be used.</p> required <code>temperature</code> <code>float</code> <p>Temperature for the expert. Defaults to 1.0.</p> <code>1.0</code> <code>model</code> <code>Optional[nn.Module]</code> <p>Model to be used for the expert. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Union[ExpertTokenizer, PreTrainedTokenizerBase]]</code> <p>Tokenizer to be used for the expert. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to be used for the expert. Defaults to 'cpu'.</p> <code>'cpu'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the expert name is not found.</p> <p>Returns:</p> Name Type Description <code>expert</code> <code>Expert</code> <p>An instance of the expert.</p>"},{"location":"api/experts/","title":"evo_prot_grad.experts","text":""},{"location":"api/experts/#expert","title":"Expert","text":""},{"location":"api/experts/#evo_prot_grad.experts.base_experts.Expert","title":"<code>evo_prot_grad.experts.base_experts.Expert</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Defines a common interface for any type of expert.</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.Expert._tokenize","title":"<code>_tokenize(inputs: List[str]) -&gt; Any</code>  <code>abstractmethod</code>","text":"<p>Tokenizes a list of protein sequences.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequences.</p> required <p>Returns:</p> Name Type Description <code>tokens</code> <code>Any</code> <p>tokenized sequence in whatever format the expert requires.</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.Expert._get_last_one_hots","title":"<code>_get_last_one_hots() -&gt; torch.Tensor</code>  <code>abstractmethod</code>","text":"<p>Returns the one-hot tensors most recently passed as input to this expert.</p> <p>The one-hot tensors are cached and accessed from  a evo_prot_grad.common.embeddings.OneHotEmbedding module, which we configure each expert to use.</p> <p>Warning</p> <p>This assumes that the desired one-hot tensors are the last tensors passed as input to the expert. If the expert is called twice, this will return the one-hot tensors from the second call. This is intended to address the issue that some experts take lists  of strings as input and internally converts them into one-hot tensors.</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.Expert._model_output_to_scalar_score","title":"<code>_model_output_to_scalar_score(model_output: torch.Tensor, **kwargs: torch.Tensor) -&gt; torch.Tensor</code>  <code>abstractmethod</code>","text":"<p>Converts the model output to a scalar score. </p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>torch.Tensor</code> <p>The output of the expert model.</p> required <code>**kwargs</code> <code>Dict</code> <p>Any additional arguments required by the expert.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>score</code> <code>torch.Tensor</code> <p>The scalar score.</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.Expert.set_wt_score","title":"<code>set_wt_score(wt_seq: str) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Sets the wildtype score value for protein wt_seq.</p> <p>Parameters:</p> Name Type Description Default <code>wt_seq</code> <code>str</code> <p>The wildtype sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.Expert.__call__","title":"<code>__call__(inputs: List[str]) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>  <code>abstractmethod</code>","text":"<p>Return the expert score for a batch of protein sequences as well as     the one-hot encoded input sequences for which a gradient can be computed.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>oh</code> <code>torch.Tensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p> <code>expert_score</code> <code>torch.Tensor</code> <p>of shape [parallel_chains]</p>"},{"location":"api/experts/#huggingfaceexpert","title":"HuggingFaceExpert","text":""},{"location":"api/experts/#evo_prot_grad.experts.base_experts.HuggingFaceExpert","title":"<code>evo_prot_grad.experts.base_experts.HuggingFaceExpert</code>","text":"<p>             Bases: <code>Expert</code></p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.HuggingFaceExpert._model_output_to_scalar_score","title":"<code>_model_output_to_scalar_score(x_oh: torch.Tensor, logits: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Returns the scalar score assuming the expert predicts a logit score for each amino acid.</p> <p>Parameters:</p> Name Type Description Default <code>x_oh</code> <code>torch.Tensor</code> <p>(torch.Tensor) of shape [parallel_chains, seq_len, vocab_size]</p> required <code>logits</code> <code>torch.Tensor</code> <p>(torch.Tensor) of shape [parallel_chains, seq_len, vocab_size]</p> required <p>Returns:</p> Name Type Description <code>score</code> <code>torch.Tensor</code> <p>of shape [parallel_chains]</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.HuggingFaceExpert.set_wt_score","title":"<code>set_wt_score(wt_seq: str) -&gt; None</code>","text":"<p>Sets the score value for wildtype protein wt_seq.</p> <p>Parameters:</p> Name Type Description Default <code>wt_seq</code> <code>str</code> <p>The wildtype sequence.</p> required"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.HuggingFaceExpert.__call__","title":"<code>__call__(inputs: List[str]) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Returns the one-hot sequences and expert score. Assumes the PLM model predicts a logit score for each amino acid.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>oh</code> <code>torch.Tensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p> <code>expert_score</code> <code>torch.Tensor</code> <p>of shape [parallel_chains]</p>"},{"location":"api/experts/#bertexpert","title":"BERTExpert","text":""},{"location":"api/experts/#evo_prot_grad.experts.bert_expert.BERTExpert","title":"<code>evo_prot_grad.experts.bert_expert.BERTExpert</code>","text":"<p>             Bases: <code>HuggingFaceExpert</code></p> <p>Expert sub-class for BERT-style HuggingFace protein language models. Implements abstract methods <code>_get_last_one_hots</code> and <code>_tokenize</code>. Swaps out the <code>BertForMaskedLM.bert.embeddings.word_embeddings</code> layer for a <code>evo_prot_grad.common.embeddings.OneHotEmbedding</code> layer.</p>"},{"location":"api/experts/#evo_prot_grad.experts.bert_expert.BERTExpert._tokenize","title":"<code>_tokenize(inputs) -&gt; BatchEncoding</code>","text":"<p>Convert inputs to a format suitable for the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>batch_encoding</code> <code>BatchEncoding</code> <p>A BatchEncoding object.</p>"},{"location":"api/experts/#evo_prot_grad.experts.bert_expert.BERTExpert._get_last_one_hots","title":"<code>_get_last_one_hots() -&gt; torch.Tensor</code>","text":"<p>Returns the one-hot tensors most recently passed as input.</p> <p>Returns:</p> Name Type Description <code>one_hots</code> <code>torch.Tensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p>"},{"location":"api/experts/#causallmexpert","title":"CausalLMExpert","text":""},{"location":"api/experts/#evo_prot_grad.experts.causallm_expert.CausalLMExpert","title":"<code>evo_prot_grad.experts.causallm_expert.CausalLMExpert</code>","text":"<p>             Bases: <code>HuggingFaceExpert</code></p> <p>Expert sub-class for autoregressive (causal) HuggingFace protein language models. Implements abstract methods <code>_get_last_one_hots</code> and <code>_tokenize</code>. Swaps out the <code>AutoModelForCausalLM.transformer.embedding</code> layer for a <code>evo_prot_grad.common.embeddings.OneHotEmbedding</code> layer.</p>"},{"location":"api/experts/#evo_prot_grad.experts.causallm_expert.CausalLMExpert._tokenize","title":"<code>_tokenize(inputs: List[str]) -&gt; BatchEncoding</code>","text":"<p>Convert inputs to a format suitable for the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>batch_encoding</code> <code>BatchEncoding</code> <p>A BatchEncoding object.</p>"},{"location":"api/experts/#evo_prot_grad.experts.causallm_expert.CausalLMExpert._get_last_one_hots","title":"<code>_get_last_one_hots()</code>","text":"<p>Returns the one-hot tensors most recently passed as input.</p>"},{"location":"api/experts/#esmexpert","title":"EsmExpert","text":""},{"location":"api/experts/#evo_prot_grad.experts.esm_expert.EsmExpert","title":"<code>evo_prot_grad.experts.esm_expert.EsmExpert</code>","text":"<p>             Bases: <code>HuggingFaceExpert</code></p> <p>Expert baseclass for HuggingFace protein language models from the ESM family. Implements abstract methods <code>_get_last_one_hots</code> and <code>_tokenize</code>. Swaps out the <code>EsmForMaskedLM.esm.embeddings.word_embeddings</code> layer for a <code>evo_prot_grad.common.embeddings.OneHotEmbedding</code> layer.</p>"},{"location":"api/experts/#evo_prot_grad.experts.esm_expert.EsmExpert._tokenize","title":"<code>_tokenize(inputs: List[str]) -&gt; BatchEncoding</code>","text":"<p>Convert inputs to a format suitable for the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>batch_encoding</code> <code>BatchEncoding</code> <p>A BatchEncoding object.</p>"},{"location":"api/experts/#evo_prot_grad.experts.esm_expert.EsmExpert._get_last_one_hots","title":"<code>_get_last_one_hots() -&gt; torch.Tensor</code>","text":"<p>Returns the one-hot tensors most recently passed as input.</p>"},{"location":"api/experts/#attributeexpert","title":"AttributeExpert","text":""},{"location":"api/experts/#evo_prot_grad.experts.base_experts.AttributeExpert","title":"<code>evo_prot_grad.experts.base_experts.AttributeExpert</code>","text":"<p>             Bases: <code>Expert</code></p> <p>Interface for experts trained (typically with supervised learning) to predict an attribute (e.g., activity or stability) from one-hot encoded sequences.</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.AttributeExpert._tokenize","title":"<code>_tokenize(inputs: List[str])</code>","text":"<p>Tokenizes a list of protein sequences.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequences.</p> required"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.AttributeExpert._model_output_to_scalar_score","title":"<code>_model_output_to_scalar_score(model_outputs: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Returns the score for the given input assuming  the expert predicts a single scalar. </p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>torch.Tensor</code> <p>(torch.Tensor) of shape [parallel_chains]</p> required <p>Returns:</p> Name Type Description <code>model_outputs</code> <code>torch.Tensor</code> <p>(torch.Tensor) of shape [parallel_chains]</p>"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.AttributeExpert.set_wt_score","title":"<code>set_wt_score(wt_seq: str) -&gt; None</code>","text":"<p>Sets the score value for wildtype protein wt_seq.</p> <p>Parameters:</p> Name Type Description Default <code>wt_seq</code> <code>str</code> <p>The wildtype sequence.</p> required"},{"location":"api/experts/#evo_prot_grad.experts.base_experts.AttributeExpert.__call__","title":"<code>__call__(inputs: List[str]) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>oh</code> <code>torch.Tensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p> <code>score</code> <code>torch.Tensor</code> <p>of shape [parallel_chains]</p>"},{"location":"api/experts/#evcouplingsexpert","title":"EVCouplingsExpert","text":""},{"location":"api/experts/#evo_prot_grad.experts.evcouplings_expert.EVCouplingsExpert","title":"<code>evo_prot_grad.experts.evcouplings_expert.EVCouplingsExpert</code>","text":"<p>             Bases: <code>Expert</code></p> <p>Expert class for EVCouplings Potts models. EVCouplings lib uses the canonical alphabet by default.</p>"},{"location":"api/experts/#evo_prot_grad.experts.evcouplings_expert.EVCouplingsExpert.__init__","title":"<code>__init__(temperature: float, model: potts.EVCouplings, device: str, tokenizer: Optional[OneHotTokenizer] = None, use_without_wildtype: bool = False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>Temperature for sampling from the expert.</p> required <code>model</code> <code>potts.EVCouplings</code> <p>The model to use for the expert.</p> required <code>device</code> <code>str</code> <p>The device to use for the expert.</p> required <code>tokenizer</code> <code>Optional[OneHotTokenizer]</code> <p>The tokenizer to use for the expert. If None, uses     OneHotTokenizer(utils.CANONICAL_ALPHABET, device).</p> <code>None</code> <code>use_without_wildtype</code> <code>bool</code> <p>Whether to use the expert without the wildtype.</p> <code>False</code>"},{"location":"api/experts/#evo_prot_grad.experts.evcouplings_expert.EVCouplingsExpert.set_wt_score","title":"<code>set_wt_score(wt_seq: str) -&gt; None</code>","text":"<p>Sets the wildtype score value for protein wt_seq</p>"},{"location":"api/experts/#evo_prot_grad.experts.evcouplings_expert.EVCouplingsExpert.__call__","title":"<code>__call__(inputs: List[str]) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>oh</code> <code>torch.Tensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p> <code>expert_score</code> <code>torch.Tensor</code> <p>of shape [parallel_chains]</p>"},{"location":"api/experts/#onehotdownstreamexpert","title":"OneHotDownstreamExpert","text":""},{"location":"api/experts/#evo_prot_grad.experts.onehot_downstream_regression_expert.OneHotDownstreamRegressionExpert","title":"<code>evo_prot_grad.experts.onehot_downstream_regression_expert.OneHotDownstreamRegressionExpert</code>","text":"<p>             Bases: <code>AttributeExpert</code></p> <p>Basic one-hot regression expert.</p>"},{"location":"api/experts/#evo_prot_grad.experts.onehot_downstream_regression_expert.OneHotDownstreamRegressionExpert.__init__","title":"<code>__init__(temperature: float, model: Module, device: str, tokenizer: Optional[OneHotTokenizer] = None, use_without_wildtype: bool = False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>Temperature for sampling from the expert.</p> required <code>model</code> <code>Module</code> <p>The model to use for the expert.</p> required <code>device</code> <code>str</code> <p>The device to use for the expert.</p> required <code>tokenizer</code> <code>Optional[OneHotTokenizer]</code> <p>The tokenizer to use for the expert. If None, a OneHotTokenizer will be constructed. Defaults to None.</p> <code>None</code> <code>use_without_wildtype</code> <code>bool</code> <p>Whether to use the expert without the wildtype. Defaults to False.</p> <code>False</code>"},{"location":"api/experts/#evo_prot_grad.experts.onehot_downstream_regression_expert.OneHotDownstreamRegressionExpert.set_wt_score","title":"<code>set_wt_score(wt_seq: str) -&gt; None</code>","text":"<p>Sets the score value for wildtype protein wt_seq.</p> <p>Parameters:</p> Name Type Description Default <code>wt_seq</code> <code>str</code> <p>The wildtype sequence.</p> required"},{"location":"api/experts/#evo_prot_grad.experts.onehot_downstream_regression_expert.OneHotDownstreamRegressionExpert.__call__","title":"<code>__call__(inputs: List[str]) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>oh</code> <code>torch.Tensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p> <code>score</code> <code>torch.Tensor</code> <p>of shape [parallel_chains]</p>"},{"location":"api/models/","title":"evo_prot_grad.models","text":""},{"location":"api/models/#onehotcnn","title":"OneHotCNN","text":""},{"location":"api/models/#evo_prot_grad.models.downstream_cnn.OneHotCNN","title":"<code>evo_prot_grad.models.downstream_cnn.OneHotCNN</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>A CNN that takes one-hot encoded sequences as input.</p> <p>OneHotCNN uses 1D convolution over the one-hot encoding dimension to embed each amino acid into a vector of size matching the  sequence length, and uses length max-pooling (1D max-pooling on the sequence length dimension) to reduce this dimension to 1. The output is then fed through a linear layer to produce a single scalar output.</p>"},{"location":"api/models/#evo_prot_grad.models.downstream_cnn.OneHotCNN.__init__","title":"<code>__init__(vocab_size: int, kernel_size: int, input_size: int, dropout: int = 0.0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>the size of the vocabulary (e.g., 20).</p> required <code>kernel_size</code> <code>int</code> <p>the size of the convolutional kernel</p> required <code>input_size</code> <code>int</code> <p>the size of the input embedding</p> required <code>dropout</code> <code>float</code> <p>the dropout probability</p> <code>0.0</code>"},{"location":"api/models/#evo_prot_grad.models.downstream_cnn.OneHotCNN.forward","title":"<code>forward(x: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>one-hot tensor of shape [parallel_chains, seq_len, vocab_size]</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>torch.Tensor</code> <p>shape [parallel_chains]</p>"},{"location":"api/models/#evcouplings-potts","title":"EVCouplings Potts","text":""},{"location":"api/models/#evo_prot_grad.models.potts.EVCouplings","title":"<code>evo_prot_grad.models.potts.EVCouplings</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>EVCoupling Potts model implemented in PyTorch.</p> <p>Represents a Potts model with a single coupling matrix and a single bias vector for a specific region (i.e., subsequence) of the wild type protein sequence under directed evolution.</p>"},{"location":"api/models/#evo_prot_grad.models.potts.EVCouplings.forward","title":"<code>forward(x: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>one-hot tensor of shape [parallel_chains, seq_len, vocab_size]</p> required <p>Returns:</p> Name Type Description <code>hamiltonian</code> <code>torch.Tensor</code> <p>shape [parallel_chains]</p>"},{"location":"api/common/embeddings/","title":"evo_prot_grad.common.embeddings","text":""},{"location":"api/common/embeddings/#onehotembedding","title":"OneHotEmbedding","text":""},{"location":"api/common/embeddings/#evo_prot_grad.common.embeddings.OneHotEmbedding","title":"<code>evo_prot_grad.common.embeddings.OneHotEmbedding</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Compute the embeddings for a sequence of amino acids. Converts a sequence of amino acids to a sequence of one-hot vectors first. Caches the one-hot tensors for computing gradients with respect to the one-hot tensors.</p>"},{"location":"api/common/embeddings/#evo_prot_grad.common.embeddings.OneHotEmbedding.forward","title":"<code>forward(input_ids: torch.LongTensor) -&gt; torch.Tensor</code>","text":"<p>Compute the embeddings for a sequence of amino acids,  caching the one-hot tensors for computing gradients with respect to the one-hot tensors.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>torch.LongTensor</code> <p>Amino acid sequences of shape [batch_size, max_sequence_len].</p> required <p>Returns:</p> Name Type Description <code>embeddings</code> <code>torch.FloatTensor</code> <p>Amino acid embeddings of shape [batch_size, max_sequence_len, embedding_dim].</p>"},{"location":"api/common/embeddings/#identityembedding","title":"IdentityEmbedding","text":""},{"location":"api/common/embeddings/#evo_prot_grad.common.embeddings.IdentityEmbedding","title":"<code>evo_prot_grad.common.embeddings.IdentityEmbedding</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>A module that does nothing except store the most recent one_hots tensor.</p>"},{"location":"api/common/embeddings/#evo_prot_grad.common.embeddings.IdentityEmbedding.forward","title":"<code>forward(one_hots: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Cache the one_hots tensor and return it.</p> <p>Parameters:</p> Name Type Description Default <code>one_hots</code> <code>torch.Tensor</code> <p>A torch.FloatTensor of shape [batch_size, max_sequence_len, vocab_size].</p> required <p>Returns:</p> Name Type Description <code>one_hots</code> <code>torch.Tensor</code> <p>The same one_hots tensor that was passed in.</p>"},{"location":"api/common/sampler/","title":"evo_prot_grad.common.sampler","text":""},{"location":"api/common/sampler/#directedevolution","title":"DirectedEvolution","text":""},{"location":"api/common/sampler/#evo_prot_grad.common.sampler.DirectedEvolution","title":"<code>evo_prot_grad.common.sampler.DirectedEvolution</code>","text":"<p>Main class for plug and play directed evolution with gradient-based discrete MCMC.</p>"},{"location":"api/common/sampler/#evo_prot_grad.common.sampler.DirectedEvolution.__init__","title":"<code>__init__(experts: List[Expert], parallel_chains: int, n_steps: int, max_mutations: int, output: str = 'last', preserved_regions: Optional[List[Tuple[int, int]]] = None, wt_protein: Optional[str] = None, wt_fasta: Optional[str] = None, verbose: bool = False, random_seed: Optional[int] = None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>experts</code> <code>List[Expert]</code> <p>List of experts</p> required <code>parallel_chains</code> <code>int</code> <p>number of parallel chains</p> required <code>n_steps</code> <code>int</code> <p>number of steps to run directed evolution</p> required <code>max_mutations</code> <code>int</code> <p>maximum mutation distance from WT, disable by setting to -1.</p> required <code>output</code> <code>str</code> <p>output type, either 'best', 'last' or 'all'. Default is 'last'.</p> <code>'last'</code> <code>preserved_regions</code> <code>List[Tuple[int, int]]</code> <p>list of tuples of (start, end) of preserved regions. Default is None.</p> <code>None</code> <code>wt_protein</code> <code>str</code> <p>wt sequence as a string. Must provide one of wt_protein or wt_fasta.</p> <code>None</code> <code>wt_fasta</code> <code>str</code> <p>path to fasta file containing wt sequence. Must provide one of wt_protein or wt_fasta.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>whether to print verbose output. Default is False.</p> <code>False</code> <code>random_seed</code> <code>int</code> <p>random seed for reproducibility. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>n_steps</code> &lt; 1.</p> <code>ValueError</code> <p>if neither <code>wt_protein</code> nor <code>wt_fasta</code> is provided.</p> <code>ValueError</code> <p>if a fasta file is passed to <code>wt_protein</code> argument.</p> <code>ValueError</code> <p>if <code>output</code> is not one of 'best', 'last' or 'all'.</p> <code>ValueError</code> <p>if no experts are provided.</p> <code>ValueError</code> <p>if any of the preserved regions are &lt; 1 amino acid long.</p>"},{"location":"api/common/sampler/#evo_prot_grad.common.sampler.DirectedEvolution.reset","title":"<code>reset()</code>","text":"<p>Initialize the parallel chains of protein sequences.</p>"},{"location":"api/common/sampler/#evo_prot_grad.common.sampler.DirectedEvolution._product_of_experts","title":"<code>_product_of_experts(inputs: List[str]) -&gt; Tuple[List[torch.Tensor], torch.Tensor]</code>","text":"<p>Compute the product of experts. Computes each expert score, multiplies it by the expert temperature, and aggregates the scores by summation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>list of protein sequences of len [parallel_chains]</p> required <p>Returns:</p> Name Type Description <code>ohs</code> <code>List[torch.Tensor]</code> <p>list of one-hot encoded sequences of len [parallel_chains]</p> <code>PoE</code> <code>torch.Tensor</code> <p>product of experts score of shape [parallel_chains]</p>"},{"location":"api/common/sampler/#evo_prot_grad.common.sampler.DirectedEvolution._compute_gradients","title":"<code>_compute_gradients(ohs: List[torch.Tensor], PoE: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Compute the gradients of the product of experts with respect to the one-hots. We put each experts one-hot input  sequence in a canonical order before summing gradients together. </p> <p>Parameters:</p> Name Type Description Default <code>ohs</code> <code>List[torch.Tensor]</code> <p>tensor one-hot embeddings of shape [parallel_chains, seq_len, vocab_size].  List is of length # experts</p> required <code>PoE</code> <code>torch.Tensor</code> <p>product of experts score of shape [parallel_chains]</p> required"},{"location":"api/common/sampler/#evo_prot_grad.common.sampler.DirectedEvolution.__call__","title":"<code>__call__() -&gt; Tuple[List[str], np.ndarray]</code>","text":"<p>Run the gradient-based MCMC sampler.</p> <p>Returns:</p> Name Type Description <code>variants</code> <code>List[str]</code> <p>list of protein sequences</p> <code>scores</code> <code>np.ndarray</code> <p>the product of expert scores for the variants</p>"},{"location":"api/common/tokenizers/","title":"evo_prot_grad.common.tokenizers","text":""},{"location":"api/common/tokenizers/#experttokenizer","title":"ExpertTokenizer","text":""},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.ExpertTokenizer","title":"<code>evo_prot_grad.common.tokenizers.ExpertTokenizer</code>","text":"<p>             Bases: <code>abc.ABC</code></p> <p>Base interface for custom Expert tokenizers.</p>"},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.ExpertTokenizer.__init__","title":"<code>__init__(alphabet: List[str])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str]</code> <p>A list of amino acid characters.</p> required"},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.ExpertTokenizer.__call__","title":"<code>__call__(seqs: List[str]) -&gt; torch.FloatTensor</code>  <code>abstractmethod</code>","text":"<p>Convert seqs to one hot tensors.</p> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>ohs</code> <code>torch.FloatTensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p>"},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.ExpertTokenizer.decode","title":"<code>decode(ohs: torch.Tensor) -&gt; List[str]</code>  <code>abstractmethod</code>","text":"<p>Convert one-hot tensors back to a list of string sequences.</p> <p>Parameters:</p> Name Type Description Default <code>ohs</code> <code>torch.Tensor</code> <p>shape [parallel_chains, seq_len, vocab_size]</p> required <p>Returns:</p> Name Type Description <code>seqs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p>"},{"location":"api/common/tokenizers/#onehottokenizer","title":"OneHotTokenizer","text":""},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.OneHotTokenizer","title":"<code>evo_prot_grad.common.tokenizers.OneHotTokenizer</code>","text":"<p>             Bases: <code>ExpertTokenizer</code></p> <p>Converts a string of amino acids into one-hot tensors.</p>"},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.OneHotTokenizer.__init__","title":"<code>__init__(alphabet: List[str])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str]</code> <p>A list of amino acid characters.</p> required"},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.OneHotTokenizer.__call__","title":"<code>__call__(seqs: List[str]) -&gt; torch.FloatTensor</code>","text":"<p>Convert seqs to one hot tensors. Assumes each sequence is the same length. Handles sequences with spaces between amino acids.</p> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p> required <p>Returns:</p> Name Type Description <code>ohs</code> <code>torch.FloatTensor</code> <p>of shape [parallel_chains, seq_len, vocab_size]</p>"},{"location":"api/common/tokenizers/#evo_prot_grad.common.tokenizers.OneHotTokenizer.decode","title":"<code>decode(ohs: torch.Tensor) -&gt; List[str]</code>","text":"<p>Convert one-hot tensors back to a list of string sequences with  a space between each amino acid.</p> <p>Parameters:</p> Name Type Description Default <code>ohs</code> <code>torch.Tensor</code> <p>shape [parallel_chains, seq_len, vocab_size]</p> required <p>Returns:</p> Name Type Description <code>seqs</code> <code>List[str]</code> <p>A list of protein sequence strings of len [parallel_chains].</p>"},{"location":"api/common/utils/","title":"evo_prot_grad.common.utils","text":""},{"location":"api/common/utils/#evo_prot_grad.common.utils.safe_logits_to_probs","title":"<code>evo_prot_grad.common.utils.safe_logits_to_probs(logits: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>safe convert logits to probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>torch.Tensor</code> <p>[parallel_chains, seq_len, vocab_size]</p> required <p>Returns:</p> Name Type Description <code>probs</code> <code>torch.Tensor</code> <p>[parallel_chains, seq_len, vocab_size]</p>"},{"location":"api/common/utils/#evo_prot_grad.common.utils.mut_distance","title":"<code>evo_prot_grad.common.utils.mut_distance(x, wt)</code>","text":"<p>Computes edit distance from x to wt</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>shape [parallel_chains, seq_len, vocab_size].</p> required <code>wt</code> <code>torch.Tensor</code> <p>shape [1, seq_len, vocab_size].</p> required <p>Returns:</p> Name Type Description <code>edits</code> <code>torch.Tensor</code> <p>shape [parallel_chains].</p>"},{"location":"api/common/utils/#evo_prot_grad.common.utils.mutation_mask","title":"<code>evo_prot_grad.common.utils.mutation_mask(x, wt)</code>","text":"<p>Allow mutations wherever mask is False. For every pos where x and wt differ, and wt is not a gap (0),   the mask is set to False. Everywhere else set to True.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>shape [parallel_chains, seq_len, vocab_size].</p> required <code>wt</code> <code>torch.Tensor</code> <p>shape [1, seq_len, vocab_size].</p> required <p>Returns:</p> Name Type Description <code>mask</code> <code>torch.BoolTensor</code> <p>shape [parallel_chains, seq_len, vocab_size].</p>"},{"location":"api/common/utils/#evo_prot_grad.common.utils.expert_alphabet_to_canonical","title":"<code>evo_prot_grad.common.utils.expert_alphabet_to_canonical(expert_alphabet: List[str], device: str) -&gt; torch.Tensor</code>","text":"<p>Create a binary matrix that shuffles the vocab dimension of a tensor    in an expert's AA alphabet order to the canonical AA alphabet order.</p> <p>Parameters:</p> Name Type Description Default <code>expert_alphabet</code> <code>List[str]</code> <p>The amino acid vocab used by the expert.</p> required <p>Returns:</p> Name Type Description <code>alignment_matrix</code> <code>torch.Tensor</code> <p>tensor of shape [len(expert_alphabet), len(CANONICAL_ALPHABET)].</p>"},{"location":"getting_started/MCMC/","title":"Gradient-based Discrete MCMC for Directed Evolution","text":"<p><code>EvoProtGrad</code> implements PPDE, the gradient-based discrete MCMC sampler introduced in our paper. The source code can be found at <code>evo_prot_grad/common/sampler.py</code>, within the <code>DirectedEvolution</code> class.</p> <p>PPDE uses gradients of the differentiable product of experts distribution with respect to the one-hot-encoded input sequence \\(X\\) to propose mutations (amino acid substitutions) during each step of MCMC. In other words, PPDE abuses PyTorch's autodiff to approximately answer the query \"given the current protein variant, what amino acid substitution will maximally increase the product of experts score?\" without actually having to try every possible amino acid substitution. This is a huge computational savings, especially when the number of possible amino acid substitutions is large. We note that this is not the typical use of autodiff, which is usually used for computing gradients of a loss function with respect to model parameters (not model inputs).</p> <p>Here's a visual illustration of a toy example with two experts \\(F(X)\\) and \\(G(X)\\): </p> <p></p> <p>The dashed and red arrows around the black dot represent possible amino acid substitutions (mutations) that can be made to the current protein variant. The red arrow points in the direction leading to the largest increase in the Product of Experts distribution (i.e., largest gradient magnitude), and is the substitution sampled by the proposal distribution. The process repeats until a maximum number of steps is reached, or a maximum number of mutations is reached (at which point the process can be restarted or terminated).</p>"},{"location":"getting_started/MCMC/#customizing-your-sampler","title":"Customizing your sampler","text":"<p>We support the following customizations to MCMC sampling, to be provided as arguments to <code>DirectedEvolution</code>. See the API documentation for more details.</p> <ul> <li><code>max_mutations</code>: The maximum number of mutations to make to the wild type protein. This is a hard limit, and the sampler will restart the chain after this number of mutations is reached.</li> <li><code>preserved_regions</code>: A list of tuples of the form <code>(start: int, end: int)</code> that specify regions of the protein sequence that should not be mutated. This is useful for specifying protein domains or other regions of interest.</li> </ul>"},{"location":"getting_started/MCMC/#limitations","title":"Limitations","text":"<p>The MCMC sampler currently only supports substitution mutations and not insertions or deletions.</p>"},{"location":"getting_started/Trying_out_EvoProtGrad/","title":"Trying out EvoProtGrad","text":"<p>First, import our library:</p> <pre><code>import evo_prot_grad\n</code></pre> <p>Create a <code>ProtBERT</code> expert from a pretrained \ud83e\udd17 HuggingFace protein language model (PLM) using <code>evo_prot_grad.get_expert</code>:</p> <p><pre><code>prot_bert_expert = evo_prot_grad.get_expert('bert', temperature = 1.0, device = 'cuda')\n</code></pre> The default BERT-style PLM in <code>EvoProtGrad</code> is <code>Rostlab/prot_bert</code>. Normally, we would need to also specify the model and tokenizer. When using a default PLM expert, we automatically pull these from the HuggingFace Hub. The temperature parameter rescales the expert scores and can be used to trade off the importance of different experts. For masked language models like <code>prot_bert</code>, we score variant sequences with the sum of amino acid log probabilities by default.</p> <p>Then, we create an instance of <code>DirectedEvolution</code> and run the search, returning a list of the best variant per Markov chain (as measured by the <code>prot_bert</code> expert):</p> <pre><code>variants, scores = evo_prot_grad.DirectedEvolution(\nwt_fasta = 'test/gfp.fasta',    # path to wild type fasta file\noutput = 'best',                # return best, last, all variants    \nexperts = [prot_bert_expert],   # list of experts to compose\nparallel_chains = 1,            # number of parallel chains to run\nn_steps = 20,                   # number of MCMC steps per chain\nmax_mutations = 10,             # maximum number of mutations per variant\nverbose = True                  # print debug info to command line\n)()\n</code></pre> <p>This class implements PPDE, the gradient-based discrete MCMC sampler introduced in our paper.</p>"},{"location":"getting_started/Trying_out_EvoProtGrad/#specifying-the-model-and-tokenizer","title":"Specifying the model and tokenizer","text":"<p>To load a HuggingFace expert with a specific model and tokenizer, provide them as arguments to <code>get_expert</code>:</p> <pre><code>from transformers import AutoTokenizer, EsmForMaskedLM\nesm2_expert = evo_prot_grad.get_expert(\n'esm',\nmodel = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\"),\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\"),\ntemperature = 1.0,\ndevice = 'cuda')\n</code></pre>"},{"location":"getting_started/Trying_out_EvoProtGrad/#composing-2-experts","title":"Composing 2+ Experts","text":"<p>You can compose multiple experts by passing multiple experts to <code>DirectedEvolution</code> as a list. As an example, we provide a ConvNet-based expert that predicts the fluorescence of GFP variants in the HuggingFace Hub:</p> <pre><code>import evo_prot_grad\nfrom transformers import AutoModel\nprot_bert_expert = evo_prot_grad.get_expert('bert', temperature = 1.0, device = 'cuda')\n# onehot_downstream_regression are experts that predict a downstream scalar property\n# from a one-hot encoding of the protein sequence\nfluorescence_expert = evo_prot_grad.get_expert(\n'onehot_downstream_regression',\ntemperature = 1.0,\nmodel = AutoModel.from_pretrained('NREL/avGFP-fluorescence-onehot-cnn',\ntrust_remote_code=True),\ndevice = 'cuda')\nvariants, scores = evo_prot_grad.DirectedEvolution(\nwt_fasta = 'test/gfp.fasta',\noutput = 'best',\nexperts = [prot_bert_expert, fluorescence_expert],\nparallel_chains = 1,\nn_steps = 100,              \nmax_mutations = 10\n)()\n</code></pre>"},{"location":"getting_started/experts/","title":"Experts","text":"<p>In EvoProtGrad, an expert has a tokenizer for converting lists of strings into Torch one-hot tensors and a differentiable model that takes as input a one-hot-encoded protein sequence and returns a score. The idea is to combine multiple experts together for directed evolution, since some experts may correlate with the desired attribute(s) you wish to optimize while other experts help steer search away from deleterious mutations. We provide a few  experts in <code>evo_prot_grad/experts</code> that you can use out of the box, such as:</p> <p>Protein Language Models (PLMs)</p> <ul> <li><code>bert</code>, BERT-style PLMs, default: <code>Rostlab/prot_bert</code></li> <li><code>causallm</code>, CausalLM-style PLMs, default: <code>lightonai/RITA_s</code></li> <li><code>esm</code>, ESM-style PLMs, default: <code>facebook/esm2_t6_8M_UR50D</code></li> </ul> <p>Potts models</p> <ul> <li><code>evcouplings</code></li> </ul> <p>and an generic expert for supervised downstream regression models</p> <ul> <li><code>onehot_downstream_regression</code></li> </ul>"},{"location":"getting_started/experts/#what-is-a-product-of-experts","title":"What is a Product of Experts?","text":"<p><code>EvoProtGrad</code> combines multiple experts into a Product of Experts. Formally, each expert defines a (typically unnormalized) probability distribution over protein sequences. High probability can correspond to high fitness or high evolutionary density.  A Product of Experts is the product of these distributions, normalized by an unknown partition function \\(Z\\). For example, if we have two experts \\(F\\) and \\(G\\), the Product of Experts is:</p> \\[ P(X) = \\frac{1}{Z}  F(X) G(X)^{\\lambda} \\] <p>where \\(F(X)\\) may correspond to the probability given by an unsupervised pretrained PLM to sequence \\(X\\) and \\(G(X)\\) may correspond to the probability assigned to \\(X\\) by a supervised downstream regression model (interpreted as an unnormalized Boltzmann distribution with temperature \\(\\lambda\\)). The log probability of the experts is:</p> \\[ \\log P(X) = \\log F(X) + \\lambda \\log G(X) - \\log Z. \\] <p>In <code>EvoProtGrad</code>, the \"score\" of each expert corresponds to either \\(\\log F(X)\\) or \\(\\log G(X)\\) here. In most cases, we interpret the scalar output of a neural network as the score. The magic of the Product of Experts formulation is that it enables us to compose arbitrary numbers of experts, essentially allowing us to \"plug and play\" with different experts to guide the search.</p> <p>In actuality, instead of just searching for a protein variant that maximizes \\(P(X)\\), <code>EvoProtGrad</code> uses gradient-based discrete MCMC to sample from \\(P(X)\\). MCMC is necessary for sampling from \\(P(X)\\) because it is impractical to compute the partition function \\(Z\\) exactly. Uniquely to <code>EvoProtGrad</code>, as long all experts are differentiable, our sampler can use the gradient of \\(\\log F(X) + \\lambda \\log G(X)\\) with respect to the one-hot protein \\(X\\) to identify the most promising mutation to apply to \\(X\\), which vastly speeds up MCMC convergence.</p>"},{"location":"getting_started/experts/#huggingface-transformers","title":"\ud83e\udd17 HuggingFace Transformers","text":"<p><code>EvoProtGrad</code> provides a convenient interface for defining and using experts from the HuggingFace Hub. To use pretrained PLMs from the HuggingFace Hub with gradient-based discrete MCMC, we swap out each transformer's token embedding layer for a custom one-hot token embedding layer. This enables us to compute and access gradients with respect to one-hot input protein sequences.</p> <p>We provide a baseclass <code>evo_prot_grad.experts.base_experts.HuggingFaceExpert</code> which is subclassed to support various types of HuggingFace PLMs. Currently, we provide three subclasses for</p> <ul> <li>BERT-style PLMs (<code>evo_prot_grad.experts.bert_expert.BertExpert</code>)</li> <li>CausalLM-style PLMs (<code>evo_prot_grad.experts.causallm_expert.CausalLMExpert</code>)</li> <li>ESM-style PLMs (<code>evo_prot_grad.experts.esm_expert.EsmExpert</code>)</li> </ul> <p>Each HuggingFace PLM expert has to specify the model and tokenizer to use. Defaults for each type of PLM are provided. </p> <p>For example, an ESM2 expert can be instantiated with <code>evo_prot_grad.get_expert</code> with only:</p> <pre><code>esm2_expert = evo_prot_grad.get_expert('esm', temperature = 1.0, device = 'cuda')\n</code></pre> <p>using the default model <code>EsmForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")</code> and tokenizer <code>AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")</code>.</p> <p>To load the ESM2 expert with a specific model and tokenizer, provide them as arguments to <code>get_expert</code>:</p> <pre><code>from transformers import AutoTokenizer, EsmForMaskedLM\nesm2_expert = evo_prot_grad.get_expert(\n'esm',\nmodel = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\"),\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\"),\ntemperature = 1.0,\ndevice = 'cuda')\n</code></pre>"},{"location":"getting_started/experts/#evcouplings-potts","title":"EVcouplings Potts","text":"<p>Potts models are (differentiable) linear undirected graphical models that capture pairwise interactions between amino acids in a protein sequence.  They are fit to multiple sequence alignments (MSAs) of homologous sequences and can be used to score the evolutionary density of a sequence with respect to the MSA via the Hamiltonian score.  The Hamiltonian score has been demonstrated to be a good proxy for the fitness of a variant. We provide an expert that wrap Potts models from the debbiemarkslab/EVcouplings library.</p> <p>The EVcouplings expert expects an EVcouplings model <code>evo_prot_grad.models.EVCouplings</code> (which is a PyTorch re-implementation of the EVcouplings <code>Couplings</code> Potts model), whose parameters we initialize from a file in \"plmc_v2\" format produced by the <code>debbiemarkslab/plmc</code> library.</p> <p>Example:</p> <pre><code>evcouplings_model = EVCouplings(\nmodel_params_file = 'GFP_AEQVI_Sarkisyan2016-linear-15/plmc/uniref100.model_params',\nfasta_file = 'test/gfp.fasta')\nevcouplings_expert = get_expert(\n'evcouplings', \ntemperature = 1.0,\nmodel = evcouplings_model)\n</code></pre>"},{"location":"getting_started/experts/#downstream-regression","title":"Downstream Regression","text":"<p>We provide a generic expert for supervised downstream regression models and a simple 1D-Convolutional neural network (CNN) PyTorch Module (<code>evo_prot_grad.models.OneHotCNN</code>) that predicts a scalar fitness score from a one-hot-encoded protein sequence.</p> <p>To get started, we provide a pretrained OneHotCNN model trained on the Green Fluorescent Protein (GFP) dataset that can be loaded from the HuggingFace Hub:</p> <pre><code>onehotcnn_model = AutoModel.from_pretrained(\n'NREL/avGFP-fluorescence-onehot-cnn', trust_remote_code=True)\nregression_expert = get_expert(\n'onehot_downstream_regression',\ntemperature = 1.0,\nmodel = onehotcnn_model)\n</code></pre>"},{"location":"getting_started/experts/#training-your-own-downstream-regression-model","title":"Training your own downstream regression model","text":"<p>You can train a <code>OneHotCNN</code> (or your own custom PyTorch model) as you normally would any supervised regression model with a dataset of pairs of (variants, fitness/stability/etc.) labels. We provide a default tokenizer class <code>OneHotTokenizer</code> that can be used to convert a list of strings of amino acids into Torch one-hot tensors. Then, you can use the trained model as an expert in <code>EvoProtGrad</code>:</p> <pre><code># Load your trained model\nonehotcnn_model = OneHotCNN()\nonehotcnn_model.load_state_dict(torch.load('onehotcnn.pt'))\nregression_expert = get_expert(\n'onehot_downstream_regression',\ntemperature = 1.0,\nmodel = onehotcnn_model)\n</code></pre>"},{"location":"getting_started/experts/#choosing-the-expert-temperature","title":"Choosing the Expert Temperature","text":"<p>The expert temperature \\(\\lambda\\) controls the relative importance of the expert in the Product of Experts. By default it is set to 1. </p> <p>Note</p> <p>By default, the expert's score for a variant is normalized by the wild type score, i.e., we subtract the wild type score from the variant score. This is to ensure that each expert in the Product of Experts is centered around 0. If you want to use the raw expert score, set <code>use_without_wildtype = True</code> when instantiating the expert.</p> <p>If using wild type centering, then we recommend first trying temperatures of 1.0 for each \\(\\lambda\\).  We describe a simple heuristic for selecting \\(\\lambda\\) via a grid search using a small dataset of labeled variants in Section 5.2 of our paper.</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>EvoProtGrad is available on PyPI and can be installed with pip: <pre><code>pip install evo_prot_grad\n</code></pre></p> <p>If you wish to run tests or register a new expert model with EvoProtGrad, please clone this repo and install in editable mode as follows:</p> <pre><code>git clone https://github.com/NREL/EvoProtGrad.git\ncd EvoProtGrad\npip install -e .\n</code></pre>"},{"location":"getting_started/installation/#test-your-installation","title":"Test your installation","text":"<p>Run our unit tests from the base directory to ensure that everything is working properly:</p> <pre><code>python3 -m unittest\n</code></pre>"},{"location":"getting_started/tokenizers/","title":"Tokenizers","text":"<p>Each expert requires a tokenizer. The tokenizer tells <code>EvoProtGrad</code> the particular amino acid ordering the expert model was trained to use and is called internally to convert a list of strings (the variants) into Torch tensors for the expert model. </p> <ul> <li>We provide a default tokenizer class <code>OneHotTokenizer</code> that can be used to convert a list of strings of amino acids into Torch one-hot tensors in the canonical order.</li> <li>HuggingFace experts expect a tokenizer of type <code>PreTrainedTokenizerBase</code> to be provided. </li> <li>You can define your own custom tokenizer class by subclassing <code>evo_prot_grad.common.tokenizers.ExpertTokenizer</code> and implementing the <code>__call__</code> and <code>decode</code> methods. See <code>OneHotTokenizer</code> for an example.</li> </ul>"},{"location":"getting_started/tokenizers/#canonicalizing-amino-acid-sequence-order","title":"Canonicalizing amino acid sequence order","text":"<p>A subtlety with combining different one-hot protein sequence models is that the models may each use a different amino acid alphabet. For example, one model may use alphabet <code>'ACDEFGHIKLMNPQRSTVWY'</code> while another may have extra <code>&lt;start&gt;</code> and <code>&lt;end&gt;</code> tokens or use a different order. If the entries of the one-hot encoded protein sequences for each model do not align, we cannot sum their gradients together for gradient-based MCMC.</p> <p>To address this, we define a \"canonical\" amino acid ordering, <code>'ACDEFGHIKLMNPQRSTVWY'</code>, and canonicalize the one-hot encoded sequences for each expert model to this ordering. Each expert internally computes and maintains a binary matrix that maps one-hot encoded tensors between the canonical ordering and the ordering used by the expert model (the binary matrix computation code is in <code>evo_prot_grad.common.utils.expert_alphabet_to_canonical</code>). This matrix is applied to Torch one-hot tensors via matrix multiplication in a differentiable manner.</p>"}]}